#!/usr/bin/env python3
# Copyright 2020 Christian Henning
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# @title          :hnets/chunked_mlp_hnet.py
# @author         :ch
# @contact        :henningc@ethz.ch
# @created        :04/15/2020
# @version        :1.0
# @python_version :3.6.10
r"""
Chunked MLP - Hypernetwork
--------------------------

The module :mod:`hnets.chunked_mlp_hnet` contains a `Chunked Hypernetwork`, that
uses a full hypernetwork (see :class:`hnets.mlp_hnet.HMLP`) to produce one
chunk of the output weights at a time.

The hypernetwork :math:`h_\theta(e)` (with input :math:`e`) operates as follows.
The target outputs  (see
:attr:`hnets.hnet_interface.HyperNetInterface.target_shapes`) are flattened and
split into equally sized chunks. Those chunks are separately generated by an
internal full hypernetwork :math:`h'_{\theta'}(e,c)` (that is hidden from the
user), where :math:`c` denotes the chunk embedding, which are internally
maintained and chunk-specific.

Note:
    This type of hypernetwork is completely agnostic to the architecture of the
    target network. The splits happen at arbitrary locations in the flattened
    target network weight vector.
"""
import numpy as np
import torch
import torch.nn as nn

from hnets.hnet_interface import HyperNetInterface
from hnets.mlp_hnet import HMLP

class ChunkedHMLP(nn.Module, HyperNetInterface):
    """Implementation of a `chunked fully-connected hypernet`.

    The ``target_shapes`` will be flattened and split into chunks of size
    ``chunk_size``. In total, there will be
    ``np.ceil(self.num_outputs/chunk_size)`` chunks, where the last chunk
    produced might contain a remainder that is discarded.

    Each chunk has it's own `chunk embedding` that is fed into the underlying
    hypernetwork.

    Note:
        It is possible to set ``uncond_in_size`` and ``cond_in_size`` to zero
        if ``cond_chunk_embs`` is ``True``.

    Attributes:
        num_chunks (int): The number of chunks that make up the final hypernet
            output. This also corresponds to the number of chunk embeddings
            required per forward sweep.
        chunk_emb_size (int): See constructor argument ``chunk_emb_size``.

    Args:
        (....): See constructor arguments of class
            :class:`hnets.mlp_hnet.HMLP`.
        chunk_size (int): The chunk size, i.e, the number of weights produced by
            single the internally maintained instance of a full hypernet
            (see :class:`hnets.mlp_hnet.HMLP`) at a time (i.e., per chunk
            embedding).
        chunk_emb_size (int): The size of a chunk embedding.

            Note:
                Embeddings will be initialized with a normal distribution using
                zero mean and unit variance.
        cond_chunk_embs (bool): Consider chunk embeddings to be conditional.
            In this case, there will be a different set of chunk embeddings per
            condition (specified via ``num_cond_embs``).

            If ``False``, there will be a total of :attr:`num_chunks` chunk
            embeddings that are maintained within :attr:`hnets.hnet_interface.\
HyperNetInterface.unconditional_param_shapes`. If ``True``, there will be
            ``num_cond_embs * self.num_chunks`` chunk embeddings that are
            maintained within :attr:`hnets.hnet_interface.\
HyperNetInterface.conditional_param_shapes`. However, if ``num_cond_embs == 0``,
            then chunk embeddings have to be provided in a special way to the
            :meth:`forward` method (see the corresponding argument ``weights``).
    """
    def __init__(self, target_shapes, chunk_size, chunk_emb_size=8,
                 cond_chunk_embs=False, uncond_in_size=0, cond_in_size=8,
                 layers=(100, 100), verbose=True, activation_fn=torch.nn.ReLU(),
                 use_bias=True, no_uncond_weights=False, no_cond_weights=False,
                 num_cond_embs=1, dropout_rate=-1, use_spectral_norm=False,
                 use_batch_norm=False):
        # FIXME find a way using super to handle multiple inheritance.
        nn.Module.__init__(self)
        HyperNetInterface.__init__(self)

        assert isinstance(chunk_size, int) and chunk_size > 0
        assert isinstance(chunk_emb_size, int) and chunk_emb_size > 0

        ### Make constructor arguments internally available ###
        self._chunk_size = chunk_size
        self._chunk_emb_size = chunk_emb_size
        self._cond_chunk_embs = cond_chunk_embs
        self._uncond_in_size = uncond_in_size
        self._cond_in_size = cond_in_size
        self._no_uncond_weights = no_uncond_weights
        self._no_cond_weights = no_cond_weights
        self._num_cond_embs = num_cond_embs

        ### Create underlying full hypernet ###
        # Note, even if chunk embeddings are considered conditional, they
        # are maintained in this object and just fed as an external input to the
        # underlying hnet.
        hnet_uncond_in_size = uncond_in_size + chunk_emb_size
        hnet_num_cond_embs = num_cond_embs
        if cond_chunk_embs and cond_in_size == 0:
            # If there are no other conditional embeddings except the chunk
            # embeddings, we tell the underlying hnet explicitly that it doesn't
            # need to maintain any conditional weights to avoid that it will
            # throw a warning.
            hnet_num_cond_embs = 0
        self._hnet = HMLP([[chunk_size]], uncond_in_size=hnet_uncond_in_size,
            cond_in_size=cond_in_size, layers=layers, verbose=False,
            activation_fn=activation_fn, use_bias=use_bias,
            no_uncond_weights=no_uncond_weights,
            no_cond_weights=no_cond_weights, num_cond_embs=hnet_num_cond_embs,
            dropout_rate=dropout_rate, use_spectral_norm=use_spectral_norm,
            use_batch_norm=use_batch_norm)

        ### Setup attributes required by interface ###
        # Most of these attributes are taken over from `self._hnet`
        self._target_shapes = target_shapes
        self._num_known_conds = self._num_cond_embs
        self._unconditional_param_shapes_ref = \
            list(self._hnet._unconditional_param_shapes_ref)

        if self._hnet._internal_params is not None:
            self._internal_params = \
                nn.ParameterList(self._hnet._internal_params)
        self._param_shapes = list(self._hnet._param_shapes)
        self._param_shapes_meta = list(self._hnet._param_shapes_meta)
        if self._hnet._hyper_shapes_learned is not None:
            self._hyper_shapes_learned = list(self._hnet._hyper_shapes_learned)
            self._hyper_shapes_learned_ref = \
                list(self._hnet._hyper_shapes_learned_ref)
        if self._hnet._hyper_shapes_distilled is not None:
            self._hyper_shapes_distilled = \
                list(self._hnet._hyper_shapes_distilled)
        self._has_bias = self._hnet._has_bias
        self._has_fc_out = self._hnet._has_fc_out
        # Just to make that clear explicitly. We will additionally append
        # the chunk embeddings at the end of `param_shapes`.
        # We don't prepend it to the beginning, to keep conditional input
        # embeddings at the beginning.
        self._mask_fc_out = False
        self._has_linear_out = self._hnet._has_linear_out
        self._layer_weight_tensors = \
            nn.ParameterList(self._hnet._layer_weight_tensors)
        self._layer_bias_vectors = \
            nn.ParameterList(self._hnet._layer_bias_vectors)
        if self._hnet._batchnorm_layers is not None:
            self._batchnorm_layers = nn.ModuleList(self._hnet._batchnorm_layers)
        if self._hnet._context_mod_layers is not None:
            self._context_mod_layers = \
                nn.ModuleList(self._hnet._context_mod_layers)

        ### Create chunk embeddings ###
        if cond_in_size == 0 and uncond_in_size == 0 and not cond_chunk_embs:
            # Note, we could also allow this case. It would be analoguous to
            # creating a full hypernet with no unconditional input and one
            # conditional embedding. But the user can explicitly achieve that
            # as noted below.
            raise ValueError('If no external (conditional or unconditional) ' +
                             'input is provided to the hypernetwork, then ' +
                             'it can only learn a fixed output. If this ' +
                             'behavior is desired, please enable ' +
                             '"cond_chunk_embs" and set "num_cond_embs=1".')

        num_cemb_mats = 1
        no_cemb_weights = no_uncond_weights
        if cond_chunk_embs:
            num_cemb_mats = num_cond_embs
            no_cemb_weights = no_cond_weights

        self._cemb_shape = [self.num_chunks, chunk_emb_size]

        for _ in range(num_cemb_mats):
            if not no_cemb_weights:
                self._internal_params.append(nn.Parameter( \
                    data=torch.Tensor(*self._cemb_shape), requires_grad=True))
                torch.nn.init.normal_(self._internal_params[-1], mean=0.,
                                      std=1.)
            else:
                self._hyper_shapes_learned.append(self._cemb_shape)
                self._hyper_shapes_learned_ref.append(len(self.param_shapes))

            if not cond_chunk_embs:
                self._unconditional_param_shapes_ref.append( \
                    len(self.param_shapes))

            self._param_shapes.append(self._cemb_shape)
            # In principle, these embeddings also belong to the input, so we
            # just assign them as "layer" 0 (note, the underlying hnet uses the
            # same layer ID for its embeddings.
            self._param_shapes_meta.append({
                'name': 'embedding',
                'index': -1 if no_cemb_weights else \
                    len(self._internal_params)-1,
                'layer': 0,
                'info': 'chunk embeddings'
            })

        ### Finalize construction ###
        self._is_properly_setup()

        if verbose:
            print('Created Chunked MLP Hypernet with %d chunk(s) of size %d.' \
                  % (self.num_chunks, chunk_size))
            print(self)

    @property
    def num_chunks(self):
        """Getter for read-only attribute :attr:`num_chunks`."""
        return int(np.ceil(self.num_outputs / self._chunk_size))

    @property
    def chunk_emb_size(self):
        """Getter for read-only attribute :attr:`chunk_emb_size`."""
        return self._chunk_emb_size

    def forward(self, uncond_input=None, cond_input=None, cond_id=None,
                weights=None, distilled_params=None, condition=None,
                ret_format='squeezed', ext_inputs=None, task_emb=None,
                task_id=None, theta=None, dTheta=None):
        """Compute the weights of a target network.

        Args:
            (....): See docstring of method
                :meth:`hnets.mlp_hnet.HMLP.forward`.
            weights (list or dict, optional): If provided as ``dict`` and
                chunk embeddings are considered conditional (see constructor
                argument ``cond_chunk_embs``), then the additional key
                ``chunk_embs`` can be used to pass a batch of chunk embeddings.
                This option is mutually exclusive with the option of passing
                ``cond_id``. Note, if conditional inputs via ``cond_input`` are
                expected, then the batch sizes must agree.

                A batch of chunk embeddings is expected to be tensor of shape
                ``[B, num_chunks, chunk_emb_size]``, where ``B`` denotes the
                batch size.

        Returns:
            (list or torch.Tensor): See docstring of method
            :meth:`hnets.hnet_interface.HyperNetInterface.forward`.
        """
        cond_chunk_embs = None
        if isinstance(weights, dict):
            if 'chunk_embs' in weights.keys():
                cond_chunk_embs = weights['chunk_embs']
                if not self._cond_chunk_embs:
                    raise ValueError('Key "chunk_embs" for argument ' +
                                     '"weights" is only allowed if chunk ' +
                                     'embeddings are conditional.')
                assert len(cond_chunk_embs.shape) == 3 and \
                    np.all(np.equal(cond_chunk_embs.shape[1:],
                                    [self.num_chunks, self.chunk_emb_size]))

                if cond_id is not None:
                    raise ValueError('Option "cond_id" is mutually exclusive ' +
                                     'with key "chunk_embs" for argument ' +
                                     '"weights".')
                assert cond_input is None or \
                    cond_input.shape[0] == cond_chunk_embs.shape[0]

                # Remove `chunk_embs` from dictionary, since upper class parser
                # doesn't know how to deal with it.
                del weights['chunk_embs']
                if len(weights.keys()) == 0: # Empty dictionary.
                    weights = None

        if cond_input is not None and self._cond_chunk_embs and \
                cond_chunk_embs is None:
            raise ValueError('Conditional chunk embeddings have to be ' +
                             'provided via "weights" if "cond_input" is ' +
                             'specified.')

        _input_required = self._cond_in_size > 0 or self._uncond_in_size > 0
        # We parse `cond_id` afterwards if chunk embeddings are also
        # conditional.
        if self._cond_chunk_embs:
            _parse_cond_id_fct = lambda x, y, z: None
        else:
            _parse_cond_id_fct = None

        uncond_input, cond_input, uncond_weights, cond_weights = \
            self._preprocess_forward_args(_input_required=_input_required,
                _parse_cond_id_fct=_parse_cond_id_fct,
                uncond_input=uncond_input, cond_input=cond_input,
                cond_id=cond_id, weights=weights,
                distilled_params=distilled_params, condition=condition,
                ret_format=ret_format, ext_inputs=ext_inputs, task_emb=task_emb,
                task_id=task_id, theta=theta, dTheta=dTheta)

        ### Translate IDs to conditional inputs ###
        if cond_id is not None and self._cond_chunk_embs:
            assert cond_input is None and cond_chunk_embs is None
            cond_id = [cond_id] if isinstance(cond_id, int) else cond_id

            if cond_weights is None:
                raise ValueError('Forward option "cond_id" can only be ' +
                                 'used if conditional parameters are ' +
                                 'maintained internally or passed to the ' +
                                 'forward method via option "weights".')

            cond_chunk_embs = []
            cond_input = [] if self._cond_in_size > 0 else None

            for i, cid in enumerate(cond_id):
                if cid < 0 or cid >= self._num_cond_embs:
                    raise ValueError('Condition %d not existing!' % (cid))

                # Note, we do not necessarily have conditional embeddings.
                if self._cond_in_size > 0:
                    cond_input.append(cond_weights[cid])

                cond_chunk_embs.append( \
                    cond_weights[-self._num_cond_embs+cid])

            if self._cond_in_size > 0:
                cond_input = torch.stack(cond_input, dim=0)
            cond_chunk_embs = torch.stack(cond_chunk_embs, dim=0)

        ### Assemble hypernetwork input ###
        batch_size = None
        if cond_input is not None:
            batch_size = cond_input.shape[0]
        if cond_chunk_embs is not None:
            assert batch_size is None or batch_size == cond_chunk_embs.shape[0]
            batch_size = cond_chunk_embs.shape[0]
        if uncond_input is not None:
            if batch_size is None:
                batch_size = uncond_input.shape[0]
            else:
                assert batch_size == uncond_input.shape[0]
        assert batch_size is not None

        chunk_embs = None
        if self._cond_chunk_embs:
            assert cond_chunk_embs is not None and \
                len(cond_chunk_embs.shape) == 3
            assert self._cond_in_size == 0 or cond_input is not None
            chunk_embs = cond_chunk_embs
        else:
            assert cond_chunk_embs is None
            chunk_embs = uncond_weights[-1]
            # Insert batch dimension.
            chunk_embs = chunk_embs.expand(batch_size, self.num_chunks,
                                           self.chunk_emb_size)

        # We now have the following setup:
        # cond_input: [batch_size, cond_in_size] or None
        # uncond_input: [batch_size, uncond_in_size] or None
        # chunk_embs: [batch_size, num_chunks, chunk_emb_size]

        # We now first copy the hypernet inputs for each chunk, arriving at
        # cond_input: [batch_size, num_chunks, cond_in_size] or None
        # uncond_input: [batch_size, num_chunks, uncond_in_size] or None
        if cond_input is not None:
            cond_input = cond_input.reshape(batch_size, 1, -1)
            cond_input = cond_input.expand(batch_size, self.num_chunks,
                                           self._cond_in_size)
        if uncond_input is not None:
            uncond_input = uncond_input.reshape(batch_size, 1, -1)
            uncond_input = uncond_input.expand(batch_size, self.num_chunks,
                                               self._uncond_in_size)
            # The chunk embeddings are considered unconditional inputs to the
            # underlying hypernetwork.
            uncond_input = torch.cat([uncond_input, chunk_embs], dim=2)
        else:
            uncond_input = chunk_embs

        # Now we build one big batch for the underlying hypernetwork, with
        # batch size: batch_size * num_chunks.
        if cond_input is not None:
            cond_input = cond_input.reshape(batch_size * self.num_chunks, -1)
        uncond_input = uncond_input.reshape(batch_size * self.num_chunks, -1)

        ### Weight of underlying hypernetwork ###
        weights = dict()
        if cond_weights is not None and self._cond_chunk_embs:
            weights['cond_weights'] = cond_weights[:-self._num_cond_embs]
        elif cond_weights is not None:
            weights['cond_weights'] = cond_weights
        assert uncond_weights is not None
        if self._cond_chunk_embs:
            weights['uncond_weights'] = uncond_weights
        else:
            weights['uncond_weights'] = uncond_weights[:-1]

        ### Process chunks ###
        hnet_out = self._hnet.forward(uncond_input=uncond_input,
                cond_input=cond_input, cond_id=None, weights=weights,
                distilled_params=distilled_params, condition=condition,
                ret_format='flattened')
        assert np.all(np.equal(hnet_out.shape, [batch_size * self.num_chunks,
                                                self._chunk_size]))

        # FIXME We can skip this line, right?
        hnet_out = hnet_out.view(batch_size, self.num_chunks, self._chunk_size)
        # Concatenate individual chunks.
        hnet_out = hnet_out.view(batch_size, self.num_chunks * self._chunk_size)

        ### Assemble hypernet output ###
        ret = self._flat_to_ret_format(hnet_out, ret_format)

        return ret

    def distillation_targets(self):
        """Targets to be distilled after training.

        See docstring of abstract super method
        :meth:`mnets.mnet_interface.MainNetInterface.distillation_targets`.

        Returns:
            See :meth:`hnets.mlp_hnet.HMLP.distillation_targets`.
        """
        # We don't have any additional distillation targets. We also just pass
        # `distilled_params` to the underlying hypernetwork in the `forward`
        # method.
        return self._hnet.distillation_targets

    def apply_chunked_hyperfan_init(self, method='in', use_xavier=False,
                                    uncond_var=1., cond_var=1., eps=1e-5,
                                    cemb_normal_init=False):
        """Not implemented yet!"""
        # TODO Translate from old hypernet implementation and take meta
        # information of generated parameters into account.
        raise NotImplementedError()

    def get_cond_in_emb(self, cond_id):
        """Get the ``cond_id``-th (conditional) input embedding.

        Args:
            (....): See docstring of method
                :meth:`hnets.mlp_hnet.HMLP.get_cond_in_emb`.

        Returns:
            (torch.nn.Parameter)
        """
        return self._hnet.get_cond_in_emb(cond_id)

    def get_chunk_emb(self, chunk_id=None, cond_id=None):
        """Get the ``chunk_id``-th chunk embedding.

        Args:
            chunk_id (int, optional): A number between 0 and :attr:`num_chunks`
                - 1. If not specified, a full chunk matrix with shape
                ``[num_chunks, chunk_emb_size]`` is returned. Otherwise,
                the ``chunk_id``-th row is returned.
            cond_id (int): Is mandatory if constructor argument
                ``cond_chunk_embs`` was set. Determines the set of chunk
                embeddings to be considered.

        Returns:
            (torch.nn.Parameter)
        """
        if self._cond_chunk_embs:
            if cond_id is None:
                raise RuntimeError('Option "cond_id" has to be set if chunk ' +
                                   'embeddings are conditional parameters!')
            if self.conditional_params is None:
                raise RuntimeError('Conditional chunk embeddings are not ' +
                                   'internally maintained!')
            if not isinstance(cond_id, int) or cond_id < 0 or \
                    cond_id >= self._num_cond_embs:
                raise RuntimeError('Option "cond_id" must be between 0 and ' +
                                   '%d!' % (self._num_cond_embs-1))
            # Note, the last `self._num_cond_embs` params are chunk embeddings.
            chunk_embs = self.conditional_params[-self._num_cond_embs+cond_id]
        else:
            assert cond_id is None
            if self.unconditional_params is None:
                raise RuntimeError('Chunk embeddings are not internally ' +
                                   'maintained!')
            chunk_embs = self.unconditional_params[-1]

        if chunk_id is None:
            return chunk_embs
        else:
            if not isinstance(chunk_id, int) or chunk_id < 0 or \
                    chunk_id >= self.num_chunks:
                raise RuntimeError('Option "chunk_id" must be between 0 and ' +
                                   '%d!' % (self.num_chunks-1))

            return chunk_embs[chunk_id, :]

if __name__ == '__main__':
    pass